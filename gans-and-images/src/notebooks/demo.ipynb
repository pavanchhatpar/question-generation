{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# example of loading the mnist dataset\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "try: \n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True) \n",
    "except: \n",
    "  # Invalid device or cannot modify virtual devices once initialized. \n",
    "  print(\"fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (60000, 28, 28) (60000,)\n",
      "Test (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# load the images into memory\n",
    "(trainX, trainy), (testX, testy) = load_data()\n",
    "# summarize the shape of the dataset\n",
    "print('Train', trainX.shape, trainy.shape)\n",
    "print('Test', testX.shape, testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(25):\n",
    "\t# define subplot\n",
    "\tpyplot.subplot(5, 5, 1 + i)\n",
    "\t# turn off axis\n",
    "\tpyplot.axis('off')\n",
    "\t# plot raw pixel data\n",
    "\tpyplot.imshow(trainX[i], cmap='gray_r')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "\tmodel = Sequential()\n",
    "#     model.add(Flatten())\n",
    "\tmodel.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\tmodel.add(Dropout(0.4))\n",
    "\tmodel.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "#     model.add(Dense(128))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\tmodel.add(Dropout(0.4))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\treturn model\n",
    " \n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "\tmodel = Sequential()\n",
    "\t# foundation for 7x7 image\n",
    "\tn_nodes = 128 * 7 * 7\n",
    "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\tmodel.add(Reshape((7, 7, 128)))\n",
    "\t# upsample to 14x14\n",
    "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# upsample to 28x28\n",
    "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\tmodel.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
    "\treturn model\n",
    " \n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\td_model.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(g_model)\n",
    "\t# add the discriminator\n",
    "\tmodel.add(d_model)\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\treturn model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare mnist training images\n",
    "def load_real_samples():\n",
    "\t# load mnist dataset\n",
    "\t(trainX, _), (_, _) = load_data()\n",
    "\t# expand to 3d, e.g. add channels dimension\n",
    "\tX = np.expand_dims(trainX, axis=-1)\n",
    "\t# convert from unsigned ints to floats\n",
    "\tX = X.astype('float32')\n",
    "\t# scale from [0,255] to [0,1]\n",
    "\tX = X / 255.0\n",
    "\treturn X\n",
    " \n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# choose random instances\n",
    "\tix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX = dataset[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = np.ones((n_samples, 1))\n",
    "\treturn X, y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = np.random.randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\tX = g_model.predict(x_input)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = np.zeros((n_samples, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save a plot of generated images (reversed grayscale)\n",
    "def save_plot(examples, epoch, n=10):\n",
    "\t# plot images\n",
    "\tfor i in range(n * n):\n",
    "\t\t# define subplot\n",
    "\t\tpyplot.subplot(n, n, 1 + i)\n",
    "\t\t# turn off axis\n",
    "\t\tpyplot.axis('off')\n",
    "\t\t# plot raw pixel data\n",
    "\t\tpyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "\t# save plot to file\n",
    "\tfilename = 'generated_plot_e%03d.png' % (epoch+1)\n",
    "\tpyplot.savefig(filename)\n",
    "\tpyplot.close()\n",
    " \n",
    "# evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "\t# prepare real samples\n",
    "\tX_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "\t# evaluate discriminator on real examples\n",
    "\t_, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "\t# prepare fake examples\n",
    "\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "\t# evaluate discriminator on fake examples\n",
    "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "\t# summarize discriminator performance\n",
    "\tprint('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "\t# save plot\n",
    "\tsave_plot(x_fake, epoch)\n",
    "\t# save the generator model tile file\n",
    "\tfilename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "\tg_model.save(filename)\n",
    " \n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=256):\n",
    "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# enumerate batches over the training set\n",
    "\t\tfor j in range(bat_per_epo):\n",
    "\t\t\t# get randomly selected 'real' samples\n",
    "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\t\t# generate 'fake' examples\n",
    "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t\t# create training set for the discriminator\n",
    "\t\t\tX, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
    "\t\t\t# update discriminator model weights\n",
    "\t\t\td_loss, _ = d_model.train_on_batch(X, y)\n",
    "\t\t\t# prepare points in latent space as input for the generator\n",
    "\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t\t# create inverted labels for the fake samples\n",
    "\t\t\ty_gan = np.ones((n_batch, 1))\n",
    "\t\t\t# update the generator via the discriminator's error\n",
    "\t\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "\t\t\t# summarize loss on this batch\n",
    "\t\t\tprint('>%d %d/%d, d=%.3f, g=%.3f\\r' % (i+1, j, bat_per_epo, d_loss, g_loss), end='')\n",
    "\t\tprint()\n",
    "\t\t# evaluate the model performance, sometimes\n",
    "\t\tif (i+1) % 10 == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, d_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 233/234, d=0.708, g=0.712\n",
      ">2 233/234, d=0.637, g=0.761\n",
      ">3 233/234, d=0.701, g=0.688\n",
      ">4 233/234, d=0.708, g=0.689\n",
      ">5 233/234, d=0.682, g=0.727\n",
      ">6 233/234, d=0.694, g=0.720\n",
      ">7 233/234, d=0.695, g=0.753\n",
      ">8 233/234, d=0.690, g=0.706\n",
      ">9 233/234, d=0.686, g=0.771\n",
      ">10 233/234, d=0.690, g=0.707\n",
      ">Accuracy real: 56%, fake: 66%\n",
      ">11 233/234, d=0.686, g=0.718\n",
      ">12 233/234, d=0.684, g=0.698\n",
      ">13 233/234, d=0.680, g=0.725\n",
      ">14 233/234, d=0.681, g=0.718\n",
      ">15 233/234, d=0.689, g=0.693\n",
      ">16 233/234, d=0.689, g=0.701\n",
      ">17 233/234, d=0.691, g=0.695\n",
      ">18 233/234, d=0.689, g=0.722\n",
      ">19 233/234, d=0.682, g=0.704\n",
      ">20 233/234, d=0.688, g=0.681\n",
      ">Accuracy real: 93%, fake: 42%\n",
      ">21 233/234, d=0.684, g=0.669\n",
      ">22 233/234, d=0.683, g=0.759\n",
      ">23 233/234, d=0.683, g=0.699\n",
      ">24 233/234, d=0.692, g=0.730\n",
      ">25 233/234, d=0.681, g=0.741\n",
      ">26 233/234, d=0.683, g=0.709\n",
      ">27 233/234, d=0.691, g=0.661\n",
      ">28 233/234, d=0.690, g=0.764\n",
      ">29 233/234, d=0.683, g=0.734\n",
      ">30 233/234, d=0.688, g=0.669\n",
      ">Accuracy real: 99%, fake: 14%\n",
      ">31 233/234, d=0.685, g=0.661\n",
      ">32 233/234, d=0.686, g=0.751\n",
      ">33 233/234, d=0.684, g=0.763\n",
      ">34 233/234, d=0.686, g=0.740\n",
      ">35 233/234, d=0.683, g=0.717\n",
      ">36 233/234, d=0.695, g=0.669\n",
      ">37 176/234, d=0.679, g=0.709\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1522f11f604f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_real_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-1c1a9b8d1dda>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(g_model, d_model, gan_model, dataset, latent_dim, n_epochs, n_batch)\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0mX_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_real_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0;31m# generate 'fake' examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                         \u001b[0mX_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                         \u001b[0;31m# create training set for the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-116c5649fc93>\u001b[0m in \u001b[0;36mgenerate_fake_samples\u001b[0;34m(g_model, latent_dim, n_samples)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_latent_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# predict outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# create 'fake' class labels (0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1613\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m     \"\"\"\n\u001b[0;32m-> 1615\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m   def interleave(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   3956\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3957\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 3958\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   3959\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3960\u001b[0m       raise TypeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 2395\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2396\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    866\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfunc_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     func_graph = FuncGraph(name, collections=collections,\n\u001b[0;32m--> 868\u001b[0;31m                            capture_by_value=capture_by_value)\n\u001b[0m\u001b[1;32m    869\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFuncGraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, collections, capture_by_value)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mouter\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfailing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFuncGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2716\u001b[0m     \u001b[0;31m# Similarly, if one or more Session.run calls are going on, all mutate ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m     \u001b[0;31m# have to wait until all Session.run calls have finished.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2718\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_lock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlock_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroupLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_groups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2719\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# GUARDED_BY(self._lock)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_id_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# GUARDED_BY(self._lock)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/lock_util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_groups)\u001b[0m\n\u001b[1;32m     68\u001b[0m       raise ValueError(\"num_groups must be a positive integer, got {}\".format(\n\u001b[1;32m     69\u001b[0m           num_groups))\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_member_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lock)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;31m# Export the lock's acquire() and release() methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "d_model = define_discriminator()\n",
    "# create the generator\n",
    "g_model = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "# train model\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
