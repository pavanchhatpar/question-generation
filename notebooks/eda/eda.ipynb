{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_gan import cfg\n",
    "from text_gan.features import GloVeReader, NERTagger, PosTagger\n",
    "from text_gan.utils import MapReduce\n",
    "\n",
    "import en_core_web_sm\n",
    "from copynet_tf import Vocab\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_reader = GloVeReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Loading vectors: 2196017it [03:03, 11966.88it/s]\n"
    }
   ],
   "source": [
    "pretrained_vectors = embedding_reader.read(cfg.EMBS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(\n",
    "    embedding_reader.START,\n",
    "    embedding_reader.END,\n",
    "    embedding_reader.PAD,\n",
    "    embedding_reader.UNK,\n",
    "    cfg.CSEQ_LEN,\n",
    "    cfg.QSEQ_LEN\n",
    ")\n",
    "ner = NERTagger(cfg.NER_TAGS_FILE, cfg.CSEQ_LEN)\n",
    "pos = PosTagger(cfg.POS_TAGS_FILE, cfg.CSEQ_LEN)\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tfds.load(\"squad\", data_dir=\"/tf/data/tf_data\", split='train').take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utf8_decoder(x):\n",
    "        return x.decode('utf-8')\n",
    "\n",
    "def substrSearch(ans, context):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    s = -1\n",
    "    while i < len(context) and j < len(ans):\n",
    "        if context[i].text == ans[j].text:\n",
    "            s = i\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            j = 0\n",
    "            s = -1\n",
    "    return s, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f246035e9d8> and will run it as-is.\nCause: could not parse the source code:\n\n    lambda x: x['context'], num_parallel_calls=AUTOTUNE)\n\nThis error may be avoided by creating the lambda in a standalone statement.\n\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f246035e9d8> and will run it as-is.\nCause: could not parse the source code:\n\n    lambda x: x['context'], num_parallel_calls=AUTOTUNE)\n\nThis error may be avoided by creating the lambda in a standalone statement.\n\nWARNING: AutoGraph could not transform <function <lambda> at 0x7f246035e9d8> and will run it as-is.\nCause: could not parse the source code:\n\n    lambda x: x['context'], num_parallel_calls=AUTOTUNE)\n\nThis error may be avoided by creating the lambda in a standalone statement.\n\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f246035eea0> and will run it as-is.\nCause: could not parse the source code:\n\n    lambda x: x['question'], num_parallel_calls=AUTOTUNE)\n\nThis error may be avoided by creating the lambda in a standalone statement.\n\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f246035eea0> and will run it as-is.\nCause: could not parse the source code:\n\n    lambda x: x['question'], num_parallel_calls=AUTOTUNE)\n\nThis error may be avoided by creating the lambda in a standalone statement.\n\nWARNING: AutoGraph could not transform <function <lambda> at 0x7f246035eea0> and will run it as-is.\nCause: could not parse the source code:\n\n    lambda x: x['question'], num_parallel_calls=AUTOTUNE)\n\nThis error may be avoided by creating the lambda in a standalone statement.\n\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f246de1f840> and will run it as-is.\nCause: could not parse the source code:\n\n    lambda x: x['answers']['text'][0], num_parallel_calls=AUTOTUNE)\n\nThis error may be avoided by creating the lambda in a standalone statement.\n\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f246de1f840> and will run it as-is.\nCause: could not parse the source code:\n\n    lambda x: x['answers']['text'][0], num_parallel_calls=AUTOTUNE)\n\nThis error may be avoided by creating the lambda in a standalone statement.\n\nWARNING: AutoGraph could not transform <function <lambda> at 0x7f246de1f840> and will run it as-is.\nCause: could not parse the source code:\n\n    lambda x: x['answers']['text'][0], num_parallel_calls=AUTOTUNE)\n\nThis error may be avoided by creating the lambda in a standalone statement.\n\n"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_context = train.map(\n",
    "    lambda x: x['context'], num_parallel_calls=AUTOTUNE)\n",
    "train_question = train.map(\n",
    "    lambda x: x['question'], num_parallel_calls=AUTOTUNE)\n",
    "train_ans = train.map(\n",
    "    lambda x: x['answers']['text'][0], num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = MapReduce()\n",
    "train_context = train_context.as_numpy_iterator()\n",
    "train_context = mr.process(utf8_decoder, train_context)\n",
    "train_question = train_question.as_numpy_iterator()\n",
    "train_question = mr.process(utf8_decoder, train_question)\n",
    "train_ans = train_ans.as_numpy_iterator()\n",
    "train_ans = mr.process(utf8_decoder, train_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context = nlp.pipe(\n",
    "    train_context, batch_size=128, n_process=6)\n",
    "train_question = nlp.pipe(\n",
    "    train_question, batch_size=128, n_process=6)\n",
    "train_ans = nlp.pipe(\n",
    "    train_ans, batch_size=128, n_process=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(10, 10, 10)"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "training_context = []\n",
    "training_question = []\n",
    "training_ans = []\n",
    "for context, ques, ans in zip(\n",
    "        train_context, train_question, train_ans):\n",
    "    ans_start, al = substrSearch(ans, context)\n",
    "    ans_start += 1\n",
    "    if len(ques) >= 20 or ans_start == -1 or ans_start + al >= 250:\n",
    "        continue\n",
    "    training_context.append(context)\n",
    "    training_question.append(ques)\n",
    "    ans = np.zeros(cfg.CSEQ_LEN, dtype=np.uint8)\n",
    "    ans[ans_start:ans_start+al] = 1\n",
    "    training_ans.append(ans)\n",
    "len(training_context), len(training_question), len(training_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "source max ('the', 88) min ('use.:74', 1)\ntarget max ('?', 10) min ('for', 1)\n"
    }
   ],
   "source": [
    "vocab.fit(\n",
    "    training_context,\n",
    "    training_question,\n",
    "    pretrained_vectors,\n",
    "    0, 0\n",
    ")\n",
    "vocab.save(cfg.VOCAB_SAVE)\n",
    "train_cidx = vocab.transform(training_context, \"source\")\n",
    "train_ner = ner.transform(training_context)\n",
    "train_pos = pos.transform(training_context)\n",
    "train_qidx = vocab.transform(training_question, \"target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([  2,  18, 168,   9,   4, 169, 170,  22,   4, 171,   7, 172,  16,\n         4, 173,  23,  26, 174,  11,  76, 175,  22,   4, 176,  12, 177,\n        77,  35,   6,  46, 178, 179,   4, 180,   5, 181,  78,  76, 182,\n       183,   6,  46, 184, 185,   4,  79,  80,  22,  47, 186,  48, 187,\n       188,  78,   6,  36,  81, 189, 190,   4,  79, 191, 192,  82,   9,\n       193, 194,   5,  14,   9, 195, 196,   5,   4, 197,  83,  80, 198,\n       199,  35,   5, 200,  14, 201,  35, 202,  37, 203,  11,  15,  38,\n        77,   8, 204,  84,  12,  49, 205,  35,   6,   3,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0])"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "train_cidx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 2,  5,  6, 15, 16, 17, 18, 19, 20, 21,  8, 22, 23,  7, 24, 25, 26,\n       27,  4,  3])"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "train_qidx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "vocab._target['?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "training_ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cseq = cfg.CSEQ_LEN\n",
    "qseq = cfg.QSEQ_LEN\n",
    "\n",
    "def gen():\n",
    "    for cidx, ner, pos, qidx, ans in zip(\n",
    "            train_cidx, train_ner, train_pos,\n",
    "            train_qidx, training_ans):\n",
    "        yield (cidx, ans, qidx, ner, pos)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    gen,\n",
    "    (tf.int32, tf.uint8, tf.int32, tf.uint8, tf.uint8),\n",
    "    (\n",
    "        tf.TensorShape([cseq]), tf.TensorShape([cseq]),\n",
    "        tf.TensorShape([qseq]), tf.TensorShape([cseq]),\n",
    "        tf.TensorShape([cseq]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total 10\n"
    }
   ],
   "source": [
    "i = 0\n",
    "for cidx, ans, qidx, ner, pos in train_dataset:\n",
    "    i += 1\n",
    "print(\"Total\", i)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}