{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_gan import cfg, Vocab\n",
    "from text_gan.features import GloVeReader, NERTagger, PosTagger\n",
    "from text_gan.utils import MapReduce\n",
    "\n",
    "import en_core_web_sm\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_reader = GloVeReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Loading vectors: 1669210it [02:19, 11974.20it/s]\n"
    }
   ],
   "source": [
    "pretrained_vectors = embedding_reader.read(cfg.EMBS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(\n",
    "    embedding_reader.START,\n",
    "    embedding_reader.END,\n",
    "    embedding_reader.PAD,\n",
    "    embedding_reader.UNK,\n",
    "    cfg.CSEQ_LEN,\n",
    "    cfg.QSEQ_LEN\n",
    ")\n",
    "ner = NERTagger(cfg.NER_TAGS_FILE, cfg.CSEQ_LEN)\n",
    "pos = PosTagger(cfg.POS_TAGS_FILE, cfg.CSEQ_LEN)\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tfds.load(\"squad\", data_dir=\"/tf/data/tf_data\", split='train').take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(x):\n",
    "    context, question, ans = list(nlp.pipe([\n",
    "        x['context'].decode('utf-8'),\n",
    "        x['question'].decode('utf-8'),\n",
    "        x['answers']['text'][0].decode('utf-8')\n",
    "    ]))\n",
    "    del x\n",
    "    return (context, question, ans)\n",
    "\n",
    "def substrSearch(ans, context):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    s = -1\n",
    "    while i < len(context) and j < len(ans):\n",
    "        if context[i].text == ans[j].text:\n",
    "            s = i\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            j = 0\n",
    "            s = -1\n",
    "    return s, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = MapReduce()\n",
    "train_iter = train.as_numpy_iterator()\n",
    "train_tokenized = mr.process(tokenize_example, train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(10, 10, 10)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_context = []\n",
    "train_question = []\n",
    "train_ans = []\n",
    "for context, ques, ans in train_tokenized:\n",
    "    ans_start, l = substrSearch(ans, context)\n",
    "    if len(ques) >= 20 or ans_start == -1 or ans_start + l >= 250:\n",
    "        continue\n",
    "    train_context.append(context)\n",
    "    train_question.append(ques)\n",
    "    train_ans.append((ans_start, l))\n",
    "len(train_context), len(train_question), len(train_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cembs.fit(train_context)\n",
    "# qembs.fit(train_question)\n",
    "vocab.fit(\n",
    "    train_context,\n",
    "    train_question,\n",
    "    pretrained_vectors,\n",
    "    0, 0)\n",
    "train_cidx = vocab.transform(train_context, \"source\")\n",
    "train_ner = ner.transform(train_context)\n",
    "train_pos = ner.transform(train_context)\n",
    "train_qidx = vocab.transform(train_question, \"target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([  2,  18, 168,   9,   4, 169, 170,  22,   4, 171,   7, 172,  16,\n         4, 173,  23,  26, 174,  11,  76, 175,  22,   4, 176,  12, 177,\n        77,  35,   6,  46, 178, 179,   4, 180,   5, 181,  78,  76, 182,\n       183,   6,  46, 184, 185,   4,  79,  80,  22,  47, 186,  48, 187,\n       188,  78,   6,  36,  81, 189, 190,   4,  79, 191, 192,  82,   9,\n       193, 194,   5,  14,   9, 195, 196,   5,   4, 197,  83,  80, 198,\n       199,  35,   5, 200,  14, 201,  35, 202,  37, 203,  11,  15,  38,\n        77,   8, 204,  84,  12,  49, 205,  35,   6,   3,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train_cidx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 2,  5,  6, 15, 16, 17, 18, 19, 20, 21,  8, 22, 23,  7, 24, 25, 26,\n       27,  4,  3])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "train_qidx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "vocab._target['?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cseq = cfg.CSEQ_LEN\n",
    "qseq = cfg.QSEQ_LEN\n",
    "\n",
    "def gen():\n",
    "    for cidx, ner, pos, qidx, ans in zip(\n",
    "            train_cidx, train_ner, train_pos,\n",
    "            train_qidx, train_ans):\n",
    "        yield (cidx, ans, qidx, ner, pos)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    gen,\n",
    "    (tf.int32, tf.int32, tf.int32, tf.uint8, tf.uint8),\n",
    "    (\n",
    "        tf.TensorShape([cseq]), tf.TensorShape([2]),\n",
    "        tf.TensorShape([qseq]), tf.TensorShape([cseq]), tf.TensorShape([cseq]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total 10\n"
    }
   ],
   "source": [
    "i = 0\n",
    "for cidx, ans, qidx, ner, pos in train_dataset:\n",
    "    i += 1\n",
    "print(\"Total\", i)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}